{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMc5G2tE6Jlz4kFX15yG3lb",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Renan-Domingues/LearnTheBasics-Pytorch/blob/main/Tutorials_05_AutomaticDifferentiation(autograd).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Automatic Differentiation\n",
        "with torch.autograd\n",
        "\n",
        "in the training, the most frequently used algorithm is back propagation.\n",
        "In this algorithm, parameters (model weights) are adjusted according to the gradient of the loss function with respect to the given parameters.\n",
        "\n",
        "PyTorch has a buit-in differentiation endine called torch.autograd to compute those gradients"
      ],
      "metadata": {
        "id": "cbMkBvLWXoSQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "One layer neural network, with input x, parameters w and b, and some loss function. it can be defined in Pytorch in the following manner:\n"
      ],
      "metadata": {
        "id": "u-GCwFyrY3fX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "x = torch.ones(5)\n",
        "y = torch.zeros(3)\n",
        "w = torch.randn(5, 3, requires_grad=True) # requires_grad = if is True, gradients need to be computed for this Tensor\n",
        "b = torch.randn(3, requires_grad=True)\n",
        "z = torch.matmul(x, w)+b\n",
        "loss = torch.nn.functional.binary_cross_entropy_with_logits(z,y)\n"
      ],
      "metadata": {
        "id": "UaBycMPiZIx6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "in this network, w and b are parameters, which we need to optimize. So we need to compute the gradients of loss functions with respect to those variables (that's why we set the ``require_grad``)."
      ],
      "metadata": {
        "id": "gTPdRth-bLNe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"Gradient function for z = {z.grad_fn}\")\n",
        "print(f\"Gradient function for loss = {loss.grad_fn}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lTq_6w3xjIS8",
        "outputId": "53bf1620-318e-49d7-d5dd-1ed4af4adcc0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Gradient function for z = <AddBackward0 object at 0x79b61dd1ba30>\n",
            "Gradient function for loss = <BinaryCrossEntropyWithLogitsBackward0 object at 0x79b61c0a17e0>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Computing Gradients\n",
        "The optimize weights of parameters in the neural network, we need to compute the derivatives of our loss function with respect to parameters.\n",
        "To compute those derivatives, we call loss.backward(), and then retrieve the values from w.grad and b.grad."
      ],
      "metadata": {
        "id": "uuTLQ1pVjiFv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "loss.backward()\n",
        "print(w.grad)\n",
        "print(b.grad)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sziPcelAkO72",
        "outputId": "fc849383-7eb8-42b9-def4-80e57ef881d4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[0.0038, 0.0018, 0.0311],\n",
            "        [0.0038, 0.0018, 0.0311],\n",
            "        [0.0038, 0.0018, 0.0311],\n",
            "        [0.0038, 0.0018, 0.0311],\n",
            "        [0.0038, 0.0018, 0.0311]])\n",
            "tensor([0.0038, 0.0018, 0.0311])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Disable Gradient Tracking\n",
        "\n",
        "By default, all tensors with requires_grad=True are tracking their computinal history and support gradient computation. However, there are some cases we don't need to do that, when we have trained the model and just wnat to apply it ti some input data, for example\n",
        "\n",
        "in the next example we will stop tracking computations with the code torch.no_grad()"
      ],
      "metadata": {
        "id": "O3hOoq1r2FdP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "z = torch.matmul(x, w)+b\n",
        "print(z.requires_grad)\n",
        "\n",
        "with torch.no_grad():\n",
        "  z = torch.matmul(x, w)+b\n",
        "print(z.requires_grad)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hWus6_RN3But",
        "outputId": "93162cb3-2617-4a0e-cbd8-9f63cdb36d4a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "True\n",
            "False\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Another way to achieve the same result\n",
        "\n",
        "z = torch.matmul(x, w)+b\n",
        "z_det = z.detach()\n",
        "print(z_det.requires_grad)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0ARL484h3eDn",
        "outputId": "0fa56a44-cf0c-48bc-9578-bf276922b49f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "False\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Why disable gradient tracking?\n",
        "\n",
        "- To Mark some parameters in your neural network as frozen parameters\n",
        "- To speed up computation whe you are only doing the forward pass (computations on tensors that do not track gradients would be more efficient)"
      ],
      "metadata": {
        "id": "qhY5WSWK3yYW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### More on Computational Graphs\n",
        "\n",
        "Autograd keeps a record of data (tensors) and all executed operations\n",
        "\n",
        "In a forward pass, autograd does 2 things simultaneously:\n",
        "- run the requested operation to compute a result tensor\n",
        "- maintain the operation's gradient function in DAG. (DAG = Dierected Acyclic Graph)\n",
        "\n",
        "In the backward pass kicks off when .backward() is called on the DAG root. autograd then:\n",
        "- computes the gradients from each .grad_fn,\n",
        "- accumulates them in the respective tensor's .grad attribute\n",
        "- using the chain rule, propagates all the way to the leaf tensors."
      ],
      "metadata": {
        "id": "35ecUG164hOK"
      }
    }
  ]
}