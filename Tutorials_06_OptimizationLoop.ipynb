{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMUVnKlrK6PX011gUdeMDfH",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Renan-Domingues/LearnTheBasics-Pytorch/blob/main/Tutorials_06_OptimizationLoop.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Optimizing Model Parameters\n",
        "Training a model is a iterative process, in each iteration a model makes a guess about the output, calculate the error in its guess (loss), collects the derivatives od the error with respect to its parameters (in the autograd), and OPTIMIZES these parameters with gradient descent\n",
        "\n",
        "Backpropagation (this process) video = https://www.youtube.com/watch?v=tIeHLnjs5U8"
      ],
      "metadata": {
        "id": "rQWZ0OzK5-38"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# from the Dataset & Dataloaders and Buid a Model section we will use these prerequisite code\n",
        "\n",
        "import torch\n",
        "from torch import nn\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision import datasets\n",
        "from torchvision.transforms import ToTensor\n",
        "\n",
        "training_data = datasets.FashionMNIST(\n",
        "    root=\"data\",\n",
        "    train=True,\n",
        "    download=True,\n",
        "    transform=ToTensor()\n",
        ")\n",
        "\n",
        "test_data = datasets.FashionMNIST(\n",
        "    root=\"data\",\n",
        "    train=False,\n",
        "    download=True,\n",
        "    transform=ToTensor()\n",
        ")\n",
        "\n",
        "train_dataloader = DataLoader(training_data, batch_size=64) # DataLoader separa os dados  em batches\n",
        "test_dataloader = DataLoader(test_data, batch_size=64)\n",
        "\n",
        "class NeuralNetwork (nn.Module):\n",
        "  def __init__(self):\n",
        "    super().__init__()\n",
        "    self.flatten = nn.Flatten()\n",
        "    self.linear_relu_stack = nn.Sequential(\n",
        "        nn.Linear(28*28, 512),\n",
        "        nn.ReLU(),\n",
        "        nn.Linear(512, 512),\n",
        "        nn.ReLU(),\n",
        "        nn.Linear(512, 10),\n",
        "    )\n",
        "  def forward(self, x):\n",
        "    x = self.flatten(x)\n",
        "    logits = self.linear_relu_stack(x)\n",
        "    return logits\n",
        "\n",
        "model = NeuralNetwork()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iMUWGo1w7_TX",
        "outputId": "21c2fa89-295b-42bf-a70e-f1f196b19eea"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-images-idx3-ubyte.gz\n",
            "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-images-idx3-ubyte.gz to data/FashionMNIST/raw/train-images-idx3-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 26421880/26421880 [00:01<00:00, 16146618.68it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting data/FashionMNIST/raw/train-images-idx3-ubyte.gz to data/FashionMNIST/raw\n",
            "\n",
            "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-labels-idx1-ubyte.gz\n",
            "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-labels-idx1-ubyte.gz to data/FashionMNIST/raw/train-labels-idx1-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 29515/29515 [00:00<00:00, 271967.88it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting data/FashionMNIST/raw/train-labels-idx1-ubyte.gz to data/FashionMNIST/raw\n",
            "\n",
            "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-images-idx3-ubyte.gz\n",
            "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-images-idx3-ubyte.gz to data/FashionMNIST/raw/t10k-images-idx3-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 4422102/4422102 [00:00<00:00, 5079079.45it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting data/FashionMNIST/raw/t10k-images-idx3-ubyte.gz to data/FashionMNIST/raw\n",
            "\n",
            "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-labels-idx1-ubyte.gz\n",
            "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-labels-idx1-ubyte.gz to data/FashionMNIST/raw/t10k-labels-idx1-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 5148/5148 [00:00<00:00, 15853360.49it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting data/FashionMNIST/raw/t10k-labels-idx1-ubyte.gz to data/FashionMNIST/raw\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Hyperparameters\n",
        "Are adjustable parameters theat let you control the model optimization process.\n",
        "Hyperparameter values can impact model training and convergence rates\n"
      ],
      "metadata": {
        "id": "mKYmqX5as_wb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "we define the following hyperparameters for training:\n",
        "- Number of epochs = the number of times to iterate over the dataset\n",
        "- Batch Size = the number of data samples propagated through the network before the parameters are update\n",
        "- Learning Rate - how much to update models parameters at each batch/epoch. Smaller values yield slow learning speed, while large values may result in unpredictable behavior during training."
      ],
      "metadata": {
        "id": "cVzpzhJjt9r8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "learning_rate = 1e-3\n",
        "batch_size = 64\n",
        "epochs = 5"
      ],
      "metadata": {
        "id": "PHZc7_vxuw3D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Optimization Loop\n",
        "Once we set our hyperparameters, we can train and optimize our model with the optimization loop.\n",
        "Each interation of the loop is called an epoch.\n",
        "\n",
        "Each epoch consists in 2 main parts:\n",
        "- The training loop = iterate over the training dataset and try to converge to optimal parameters.\n",
        "- the validation/test loop = iterate over the test dataset to check if model performace is improving"
      ],
      "metadata": {
        "id": "FViZ2NNlu-Tx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Loss Function\n",
        "When presenting with the training data, our untrained network is likely  not to give a correct answer.\n",
        "The LOSS FUNCTION mesures the degree of dissimilarity of obtained result to the target value, and it is the loss function that we want to minimize during training.\n",
        "To calculate the loss, we make predctions using inputs of our given data sample and compare against the true data label value"
      ],
      "metadata": {
        "id": "gIeWYAVUwcZV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Common loss function\n",
        "nn.MSELoss = https://pytorch.org/docs/stable/generated/torch.nn.MSELoss.html#torch.nn.MSELoss\n",
        "nn.NLLLoss = https://pytorch.org/docs/stable/generated/torch.nn.NLLLoss.html#torch.nn.NLLLoss\n",
        "nn.CrossEntropyLoss (combines nn.LogSoftmax and nn.NLLLoss) = https://pytorch.org/docs/stable/generated/torch.nn.CrossEntropyLoss.html#torch.nn.CrossEntropyLoss"
      ],
      "metadata": {
        "id": "5gxFNabqxivM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "We pass our model's output logits to nn.CrossEntropyLoss, which will normalize the logits and compute prediction error.\n",
        "'''\n",
        "\n",
        "loss_fn = nn.CrossEntropyLoss()"
      ],
      "metadata": {
        "id": "tO5PXq8WySXD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Optimizer\n",
        "It adjust the model parameters to reduce model error in each training step.\n",
        "``Optimization algorithms`` define how this process os performed (in this example we use Stochastic Gradient Descent).\n",
        "All the optimization logic is encapsulated in the optimizer object. Here we use the SGD optimizer"
      ],
      "metadata": {
        "id": "bQAhgQkJy3kZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We initialize the optimizer by registering the model's parameters that need to be trained, and passing in the learning rate hyperparameter."
      ],
      "metadata": {
        "id": "9VxZbD0t0B9J"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)"
      ],
      "metadata": {
        "id": "UBd8icZb0lxW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Inside the training loop, optimization happens in three steps:\n",
        "- Call optimizer.zero_grad() to resset the gradients of model parameters. (Gradients by default add up, to precent double-couting, we explicitly zero them at each interation)\n",
        "- Backpropagate the prediction loss with a call to loss.backward(). Pytorch deposits the gradients od the loss w.r.t each parameter.\n",
        "- Once we have our gradients, we call optimizer.step() to adjust parameters by the gradients collected in the backward pass."
      ],
      "metadata": {
        "id": "lmynk4ns1LNM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Full Implamentation\n",
        "We define train_loop that loops over out optimization code, and test_loop that evaluates the model's performance against our test data."
      ],
      "metadata": {
        "id": "h4NgDBp33FGm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def train_loop(dataloader, model, loss_fn, optimizer):\n",
        "  size = len(dataloader.dataset)\n",
        "\n",
        "  model.train()\n",
        "  for batch, (X, y) in enumerate(dataloader):\n",
        "    # Compute prediction and loss\n",
        "    pred = model(X)\n",
        "    loss = loss_fn(pred, y)\n",
        "\n",
        "    # Backpropagation\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    optimizer.zero_grad()\n",
        "\n",
        "    if batch % 100 == 0:\n",
        "      loss, current = loss.item(), (batch + 1) * len(X)\n",
        "      print(f\"loss: {loss:>7f} [{current:>5d}/{size:>5d}]\")\n",
        "\n",
        "def test_loop(dataloader, model, loss_fn):\n",
        "  # set the model to evaluation mode\n",
        "  model.eval()\n",
        "  size = len(dataloader.dataset)\n",
        "  num_batches = len(dataloader)\n",
        "  test_loss, correct = 0, 0\n",
        "\n",
        "  # Evaluating the model with torch.no_grad() ensures that no gradients are computed during the test mode\n",
        "  with torch.no_grad():\n",
        "    for X, y in dataloader:\n",
        "      pred = model(X)\n",
        "      test_loss += loss_fn(pred, y).item()\n",
        "      correct += (pred.argmax(1) == y).type(torch.float).sum().item()\n",
        "\n",
        "  test_loss /= num_batches\n",
        "  correct /= size\n",
        "\n",
        "  print(f\"Test Error: \\n Accuracy: {(100*correct):>0.1f}%, Avg loss: {test_loss:>8f} \\n\")"
      ],
      "metadata": {
        "id": "ViXvBrgl3igr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We initialize the loss function and optimizer, and pass it to train_loop and test_loop."
      ],
      "metadata": {
        "id": "nd53Vqsv7xjk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "loss_fn = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)\n",
        "\n",
        "epochs = 10\n",
        "for t in range(epochs):\n",
        "  print(f\"Epoch {t+1}\\n---------------------\")\n",
        "  train_loop(train_dataloader, model, loss_fn, optimizer)\n",
        "  test_loop(test_dataloader, model, loss_fn)\n",
        "  print(\"Done!\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CqbM9zJE8A51",
        "outputId": "0b6f0798-71ec-46bf-a07a-3229e3d300fc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1\n",
            "---------------------\n",
            "loss: 0.762102 [   64/60000]\n",
            "loss: 0.839100 [ 6464/60000]\n",
            "loss: 0.609797 [12864/60000]\n",
            "loss: 0.813437 [19264/60000]\n",
            "loss: 0.709475 [25664/60000]\n",
            "loss: 0.716383 [32064/60000]\n",
            "loss: 0.782769 [38464/60000]\n",
            "loss: 0.770245 [44864/60000]\n",
            "loss: 0.764624 [51264/60000]\n",
            "loss: 0.748785 [57664/60000]\n",
            "Test Error: \n",
            " Accuracy: 73.1%, Avg loss: 0.737093 \n",
            "\n",
            "Done!\n",
            "Epoch 2\n",
            "---------------------\n",
            "loss: 0.730493 [   64/60000]\n",
            "loss: 0.812447 [ 6464/60000]\n",
            "loss: 0.584114 [12864/60000]\n",
            "loss: 0.793457 [19264/60000]\n",
            "loss: 0.691530 [25664/60000]\n",
            "loss: 0.696586 [32064/60000]\n",
            "loss: 0.759712 [38464/60000]\n",
            "loss: 0.756886 [44864/60000]\n",
            "loss: 0.746206 [51264/60000]\n",
            "loss: 0.729513 [57664/60000]\n",
            "Test Error: \n",
            " Accuracy: 74.1%, Avg loss: 0.717888 \n",
            "\n",
            "Done!\n",
            "Epoch 3\n",
            "---------------------\n",
            "loss: 0.702240 [   64/60000]\n",
            "loss: 0.787967 [ 6464/60000]\n",
            "loss: 0.561694 [12864/60000]\n",
            "loss: 0.775916 [19264/60000]\n",
            "loss: 0.675603 [25664/60000]\n",
            "loss: 0.679660 [32064/60000]\n",
            "loss: 0.738291 [38464/60000]\n",
            "loss: 0.744597 [44864/60000]\n",
            "loss: 0.729929 [51264/60000]\n",
            "loss: 0.711732 [57664/60000]\n",
            "Test Error: \n",
            " Accuracy: 75.0%, Avg loss: 0.700430 \n",
            "\n",
            "Done!\n",
            "Epoch 4\n",
            "---------------------\n",
            "loss: 0.676646 [   64/60000]\n",
            "loss: 0.765275 [ 6464/60000]\n",
            "loss: 0.541865 [12864/60000]\n",
            "loss: 0.760109 [19264/60000]\n",
            "loss: 0.661378 [25664/60000]\n",
            "loss: 0.664886 [32064/60000]\n",
            "loss: 0.718162 [38464/60000]\n",
            "loss: 0.733276 [44864/60000]\n",
            "loss: 0.715314 [51264/60000]\n",
            "loss: 0.695134 [57664/60000]\n",
            "Test Error: \n",
            " Accuracy: 75.8%, Avg loss: 0.684362 \n",
            "\n",
            "Done!\n",
            "Epoch 5\n",
            "---------------------\n",
            "loss: 0.653335 [   64/60000]\n",
            "loss: 0.744207 [ 6464/60000]\n",
            "loss: 0.524056 [12864/60000]\n",
            "loss: 0.745710 [19264/60000]\n",
            "loss: 0.648640 [25664/60000]\n",
            "loss: 0.651731 [32064/60000]\n",
            "loss: 0.699088 [38464/60000]\n",
            "loss: 0.722922 [44864/60000]\n",
            "loss: 0.702161 [51264/60000]\n",
            "loss: 0.679539 [57664/60000]\n",
            "Test Error: \n",
            " Accuracy: 76.5%, Avg loss: 0.669483 \n",
            "\n",
            "Done!\n",
            "Epoch 6\n",
            "---------------------\n",
            "loss: 0.632125 [   64/60000]\n",
            "loss: 0.724532 [ 6464/60000]\n",
            "loss: 0.507982 [12864/60000]\n",
            "loss: 0.732401 [19264/60000]\n",
            "loss: 0.637230 [25664/60000]\n",
            "loss: 0.639948 [32064/60000]\n",
            "loss: 0.681201 [38464/60000]\n",
            "loss: 0.713582 [44864/60000]\n",
            "loss: 0.690431 [51264/60000]\n",
            "loss: 0.664841 [57664/60000]\n",
            "Test Error: \n",
            " Accuracy: 77.1%, Avg loss: 0.655710 \n",
            "\n",
            "Done!\n",
            "Epoch 7\n",
            "---------------------\n",
            "loss: 0.612710 [   64/60000]\n",
            "loss: 0.706291 [ 6464/60000]\n",
            "loss: 0.493445 [12864/60000]\n",
            "loss: 0.720144 [19264/60000]\n",
            "loss: 0.626994 [25664/60000]\n",
            "loss: 0.629493 [32064/60000]\n",
            "loss: 0.664460 [38464/60000]\n",
            "loss: 0.705211 [44864/60000]\n",
            "loss: 0.680062 [51264/60000]\n",
            "loss: 0.650991 [57664/60000]\n",
            "Test Error: \n",
            " Accuracy: 77.6%, Avg loss: 0.642968 \n",
            "\n",
            "Done!\n",
            "Epoch 8\n",
            "---------------------\n",
            "loss: 0.594967 [   64/60000]\n",
            "loss: 0.689277 [ 6464/60000]\n",
            "loss: 0.480254 [12864/60000]\n",
            "loss: 0.708595 [19264/60000]\n",
            "loss: 0.617771 [25664/60000]\n",
            "loss: 0.620154 [32064/60000]\n",
            "loss: 0.648894 [38464/60000]\n",
            "loss: 0.697696 [44864/60000]\n",
            "loss: 0.670973 [51264/60000]\n",
            "loss: 0.637907 [57664/60000]\n",
            "Test Error: \n",
            " Accuracy: 78.1%, Avg loss: 0.631187 \n",
            "\n",
            "Done!\n",
            "Epoch 9\n",
            "---------------------\n",
            "loss: 0.578711 [   64/60000]\n",
            "loss: 0.673529 [ 6464/60000]\n",
            "loss: 0.468130 [12864/60000]\n",
            "loss: 0.697813 [19264/60000]\n",
            "loss: 0.609369 [25664/60000]\n",
            "loss: 0.611709 [32064/60000]\n",
            "loss: 0.634404 [38464/60000]\n",
            "loss: 0.691112 [44864/60000]\n",
            "loss: 0.663072 [51264/60000]\n",
            "loss: 0.625560 [57664/60000]\n",
            "Test Error: \n",
            " Accuracy: 78.5%, Avg loss: 0.620306 \n",
            "\n",
            "Done!\n",
            "Epoch 10\n",
            "---------------------\n",
            "loss: 0.563793 [   64/60000]\n",
            "loss: 0.658970 [ 6464/60000]\n",
            "loss: 0.457012 [12864/60000]\n",
            "loss: 0.687690 [19264/60000]\n",
            "loss: 0.601703 [25664/60000]\n",
            "loss: 0.603951 [32064/60000]\n",
            "loss: 0.621012 [38464/60000]\n",
            "loss: 0.685416 [44864/60000]\n",
            "loss: 0.656209 [51264/60000]\n",
            "loss: 0.613931 [57664/60000]\n",
            "Test Error: \n",
            " Accuracy: 78.9%, Avg loss: 0.610258 \n",
            "\n",
            "Done!\n"
          ]
        }
      ]
    }
  ]
}